{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.classify.maxent import MaxentClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tuần , báo sggp tổ_chức tọa_đàm “ góp_ý dự_thả...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nghị_quyết 18 - nq / tw yêu_cầu quy_định thuế ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>, đất đánh_thuế . ( ảnh minh_họa : ttxvn ) 16/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>quy_định khung giá đất , sở_hữu nhà_đất đánh_t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>nghị_quyết 18 - nq / tw xây_dựng cơ_sở tổng_kế...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  label\n",
       "0           0  tuần , báo sggp tổ_chức tọa_đàm “ góp_ý dự_thả...     -1\n",
       "1           1  nghị_quyết 18 - nq / tw yêu_cầu quy_định thuế ...      1\n",
       "2           2  , đất đánh_thuế . ( ảnh minh_họa : ttxvn ) 16/...      1\n",
       "3           3  quy_định khung giá đất , sở_hữu nhà_đất đánh_t...      1\n",
       "4           4  nghị_quyết 18 - nq / tw xây_dựng cơ_sở tổng_kế...      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/Data_dat_dai/data_after_preprocessing.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df,test_size=0.15,random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(641, 3)\n",
      "(114, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_label_train = df_train['label'].to_list()\n",
    "list_text_train = df_train['text'].to_list()\n",
    "list_train = []\n",
    "for i in range(len(list_text_train)):\n",
    "    list_train.append((list_text_train[i].split(' '),list_label_train[i]))\n",
    "\n",
    "list_label_test = df_test['label'].to_list()\n",
    "list_text_test = df_test['text'].to_list()\n",
    "list_test = []\n",
    "for i in range(len(list_label_test)):\n",
    "    list_test.append((list_text_test[i].split(' '),list_label_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list_train[1])\n",
    "# print(list_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_dict(words_list):\n",
    "  return dict([(word, True) for word in words_list])\n",
    " \n",
    "training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in list_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_formatted = [(list_to_dict(element[0]), element[1]) for element in list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (200 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.599\n",
      "             2          -0.69032        0.786\n",
      "             3          -0.68753        0.788\n",
      "             4          -0.68476        0.789\n",
      "             5          -0.68203        0.791\n",
      "             6          -0.67932        0.794\n",
      "             7          -0.67664        0.799\n",
      "             8          -0.67399        0.799\n",
      "             9          -0.67137        0.800\n",
      "            10          -0.66877        0.802\n",
      "            11          -0.66621        0.805\n",
      "            12          -0.66366        0.807\n",
      "            13          -0.66115        0.807\n",
      "            14          -0.65866        0.808\n",
      "            15          -0.65620        0.813\n",
      "            16          -0.65376        0.816\n",
      "            17          -0.65134        0.816\n",
      "            18          -0.64895        0.817\n",
      "            19          -0.64659        0.819\n",
      "            20          -0.64425        0.819\n",
      "            21          -0.64193        0.819\n",
      "            22          -0.63963        0.819\n",
      "            23          -0.63736        0.822\n",
      "            24          -0.63511        0.825\n",
      "            25          -0.63288        0.828\n",
      "            26          -0.63067        0.833\n",
      "            27          -0.62848        0.833\n",
      "            28          -0.62632        0.835\n",
      "            29          -0.62417        0.838\n",
      "            30          -0.62204        0.838\n",
      "            31          -0.61994        0.838\n",
      "            32          -0.61785        0.839\n",
      "            33          -0.61578        0.841\n",
      "            34          -0.61373        0.841\n",
      "            35          -0.61170        0.842\n",
      "            36          -0.60969        0.842\n",
      "            37          -0.60769        0.844\n",
      "            38          -0.60571        0.844\n",
      "            39          -0.60375        0.844\n",
      "            40          -0.60181        0.844\n",
      "            41          -0.59988        0.844\n",
      "            42          -0.59797        0.846\n",
      "            43          -0.59608        0.846\n",
      "            44          -0.59420        0.847\n",
      "            45          -0.59234        0.847\n",
      "            46          -0.59049        0.850\n",
      "            47          -0.58866        0.850\n",
      "            48          -0.58684        0.850\n",
      "            49          -0.58504        0.850\n",
      "            50          -0.58326        0.852\n",
      "            51          -0.58148        0.853\n",
      "            52          -0.57973        0.855\n",
      "            53          -0.57798        0.856\n",
      "            54          -0.57625        0.856\n",
      "            55          -0.57453        0.858\n",
      "            56          -0.57283        0.860\n",
      "            57          -0.57114        0.860\n",
      "            58          -0.56946        0.863\n",
      "            59          -0.56780        0.863\n",
      "            60          -0.56615        0.863\n",
      "            61          -0.56451        0.863\n",
      "            62          -0.56288        0.864\n",
      "            63          -0.56127        0.864\n",
      "            64          -0.55966        0.864\n",
      "            65          -0.55807        0.866\n",
      "            66          -0.55649        0.867\n",
      "            67          -0.55493        0.867\n",
      "            68          -0.55337        0.867\n",
      "            69          -0.55183        0.867\n",
      "            70          -0.55029        0.867\n",
      "            71          -0.54877        0.867\n",
      "            72          -0.54726        0.867\n",
      "            73          -0.54576        0.867\n",
      "            74          -0.54427        0.871\n",
      "            75          -0.54279        0.871\n",
      "            76          -0.54132        0.872\n",
      "            77          -0.53986        0.872\n",
      "            78          -0.53841        0.874\n",
      "            79          -0.53697        0.874\n",
      "            80          -0.53555        0.875\n",
      "            81          -0.53413        0.875\n",
      "            82          -0.53272        0.875\n",
      "            83          -0.53132        0.875\n",
      "            84          -0.52993        0.875\n",
      "            85          -0.52855        0.875\n",
      "            86          -0.52717        0.875\n",
      "            87          -0.52581        0.875\n",
      "            88          -0.52446        0.878\n",
      "            89          -0.52311        0.878\n",
      "            90          -0.52178        0.878\n",
      "            91          -0.52045        0.881\n",
      "            92          -0.51913        0.881\n",
      "            93          -0.51782        0.885\n",
      "            94          -0.51652        0.886\n",
      "            95          -0.51523        0.886\n",
      "            96          -0.51394        0.888\n",
      "            97          -0.51267        0.888\n",
      "            98          -0.51140        0.888\n",
      "            99          -0.51014        0.889\n",
      "           100          -0.50889        0.889\n",
      "           101          -0.50764        0.889\n",
      "           102          -0.50640        0.891\n",
      "           103          -0.50518        0.891\n",
      "           104          -0.50395        0.891\n",
      "           105          -0.50274        0.891\n",
      "           106          -0.50153        0.891\n",
      "           107          -0.50033        0.891\n",
      "           108          -0.49914        0.891\n",
      "           109          -0.49796        0.891\n",
      "           110          -0.49678        0.889\n",
      "           111          -0.49561        0.889\n",
      "           112          -0.49445        0.889\n",
      "           113          -0.49329        0.889\n",
      "           114          -0.49214        0.889\n",
      "           115          -0.49100        0.889\n",
      "           116          -0.48987        0.891\n",
      "           117          -0.48874        0.892\n",
      "           118          -0.48761        0.892\n",
      "           119          -0.48650        0.892\n",
      "           120          -0.48539        0.892\n",
      "           121          -0.48429        0.892\n",
      "           122          -0.48319        0.892\n",
      "           123          -0.48210        0.892\n",
      "           124          -0.48102        0.892\n",
      "           125          -0.47994        0.892\n",
      "           126          -0.47887        0.892\n",
      "           127          -0.47780        0.892\n",
      "           128          -0.47674        0.892\n",
      "           129          -0.47569        0.892\n",
      "           130          -0.47464        0.892\n",
      "           131          -0.47360        0.892\n",
      "           132          -0.47257        0.892\n",
      "           133          -0.47154        0.892\n",
      "           134          -0.47051        0.892\n",
      "           135          -0.46949        0.892\n",
      "           136          -0.46848        0.892\n",
      "           137          -0.46748        0.892\n",
      "           138          -0.46647        0.892\n",
      "           139          -0.46548        0.892\n",
      "           140          -0.46449        0.892\n",
      "           141          -0.46350        0.894\n",
      "           142          -0.46252        0.895\n",
      "           143          -0.46155        0.897\n",
      "           144          -0.46058        0.897\n",
      "           145          -0.45961        0.897\n",
      "           146          -0.45865        0.899\n",
      "           147          -0.45770        0.899\n",
      "           148          -0.45675        0.899\n",
      "           149          -0.45581        0.899\n",
      "           150          -0.45487        0.899\n",
      "           151          -0.45393        0.900\n",
      "           152          -0.45301        0.902\n",
      "           153          -0.45208        0.902\n",
      "           154          -0.45116        0.902\n",
      "           155          -0.45025        0.902\n",
      "           156          -0.44934        0.902\n",
      "           157          -0.44843        0.903\n",
      "           158          -0.44753        0.903\n",
      "           159          -0.44664        0.905\n",
      "           160          -0.44574        0.906\n",
      "           161          -0.44486        0.908\n",
      "           162          -0.44398        0.908\n",
      "           163          -0.44310        0.906\n",
      "           164          -0.44222        0.906\n",
      "           165          -0.44136        0.906\n",
      "           166          -0.44049        0.906\n",
      "           167          -0.43963        0.906\n",
      "           168          -0.43877        0.906\n",
      "           169          -0.43792        0.906\n",
      "           170          -0.43707        0.906\n",
      "           171          -0.43623        0.908\n",
      "           172          -0.43539        0.908\n",
      "           173          -0.43456        0.908\n",
      "           174          -0.43373        0.908\n",
      "           175          -0.43290        0.908\n",
      "           176          -0.43208        0.908\n",
      "           177          -0.43126        0.908\n",
      "           178          -0.43044        0.908\n",
      "           179          -0.42963        0.910\n",
      "           180          -0.42882        0.910\n",
      "           181          -0.42802        0.911\n",
      "           182          -0.42722        0.911\n",
      "           183          -0.42642        0.911\n",
      "           184          -0.42563        0.911\n",
      "           185          -0.42484        0.913\n",
      "           186          -0.42406        0.913\n",
      "           187          -0.42328        0.913\n",
      "           188          -0.42250        0.913\n",
      "           189          -0.42173        0.913\n",
      "           190          -0.42096        0.913\n",
      "           191          -0.42019        0.914\n",
      "           192          -0.41943        0.914\n",
      "           193          -0.41867        0.916\n",
      "           194          -0.41791        0.916\n",
      "           195          -0.41716        0.916\n",
      "           196          -0.41641        0.917\n",
      "           197          -0.41567        0.917\n",
      "           198          -0.41493        0.917\n",
      "           199          -0.41419        0.919\n",
      "         Final          -0.41345        0.919\n",
      "  -0.058 loay_hoay==True and label is 1\n",
      "  -0.056 xã_hội_chủ_nghĩa==True and label is -1\n",
      "  -0.053 chỉnh_lý==True and label is -1\n",
      "  -0.051 2006==True and label is 1\n",
      "  -0.050 vương_đình==True and label is -1\n",
      "  -0.048 biến_đổi==True and label is -1\n",
      "  -0.048 loại_bỏ==True and label is -1\n",
      "  -0.047 xu_thế==True and label is -1\n",
      "  -0.047 khí_hậu==True and label is -1\n",
      "  -0.047 dân_tộc==True and label is -1\n"
     ]
    }
   ],
   "source": [
    "algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]\n",
    "mx = MaxentClassifier.train(training_set_formatted, algorithm,max_iter = 300)\n",
    "mx.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = mx.classify_many([test_set_formatted[i][0] for i in range(len(list_label_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxent Accuracy Score ->  85.08771929824562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.90      0.73      0.80        48\n",
      "           1       0.83      0.94      0.88        66\n",
      "\n",
      "    accuracy                           0.85       114\n",
      "   macro avg       0.86      0.83      0.84       114\n",
      "weighted avg       0.86      0.85      0.85       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Maxent Accuracy Score -> \",accuracy_score(list_label_test, y_test_pred)*100)\n",
    "print(classification_report(list_label_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
